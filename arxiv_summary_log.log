2024-01-06 19:03:44,440:INFO:

 *** The script has started *** 

2024-01-06 19:03:44,440:INFO:Python Executable: C:\ProgramData\anaconda3\envs\project1_env\python.exe
2024-01-06 19:03:45,272:INFO:Yay! Successfully fetched the arxiv.org papers. They are now contained in an XLM formal in the "papers" variable.
2024-01-06 19:03:48,721:INFO:Raw summarization response: [{'summary_text': ' ODIN: A Single Model for 2D and 3D Perception is published by Carnegie Mellon University, Stanford University and Microsoft. ODIN is a model that can segment and label both 2D RGB images and. 3D point clouds. It uses a transformer architecture that alternates between 2D within-view and 3.D cross-view information fusion. It achieves state-of-the-art performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation benchmarks.'}]
2024-01-06 19:03:48,759:INFO:Using HuggingFace's inference API to attempt to summarize the paper titled: ODIN: A Single Model for 2D and 3D Perception
2024-01-06 19:03:50,975:INFO:Raw summarization response: [{'summary_text': 'Large Language Models (LLMs) have shown to encompass a range of foundational capabilities such as commonsense and factual reasoning, world knowledge, and coherent language generation. We propose CALM—Composition to Augment Language Models. Salient features of CALM are: (i) Scales up LLMs on new tasks by ‘re-using’ existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings.'}]
2024-01-06 19:03:50,979:INFO:Using HuggingFace's inference API to attempt to summarize the paper titled: LLM Augmented LLMs: Expanding Capabilities through Composition
2024-01-06 19:03:50,979:INFO:Formated data written into full_papers_data Json file.
2024-01-06 19:03:50,979:INFO:Summaries written into summaries_data Json file.
2024-01-06 19:03:50,979:INFO:

 *** The script has ended *** 
 
2024-01-06 19:06:57,988:INFO:

 *** The script has started *** 

2024-01-06 19:06:57,988:INFO:Python Executable: C:\ProgramData\anaconda3\envs\project1_env\python.exe
2024-01-06 19:06:58,808:INFO:Yay! Successfully fetched the arxiv.org papers. They are now contained in an XLM formal in the "papers" variable.
2024-01-06 19:07:49,709:INFO:Raw summarization response: [{'summary_text': ' ODIN: A Single Model for 2D and 3D Perception is published by Carnegie Mellon University, Stanford University and Microsoft. ODIN is a model that can segment and label both 2D RGB images and. 3D point clouds. It uses a transformer architecture that alternates between 2D within-view and 3.D cross-view information fusion. It achieves state-of-the-art performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation benchmarks. It outperforms all previous works by a wide margin when the sensed 3D. point cloud is used in place of the point cloud sampled from 3D mesh. When used as the 3D perception engine in an instructable embodied agent architecture, it sets a new state of theart on the TEACh action-from-dialogue benchmark. It also opens the door to new methods, which can train and perform inference in single-view or multi-view settings with either RGB or RGB-D sensors. We propose ODIN as a model for a new type of 3D object segmentation, called OmniDimensional INstance segmentation (ODIN†, a.k.a. “Omni-Dimensional Instance Segmentation”). It is available for download at http://odin-seg.io/odin.io/. The code and checkpoints can be found at the project website https://od in-seG.io . The paper is also available as a free download athttp://www.cs.cmu.edu/research/ODIN/ODin-Seg.html. It is published in the open-source version of the book, which is available to download for free on the Mac and PC as well as the Linux and Windows versions. For more information, visit http:// www.cs-cmu-edu.com/research-ODIN. The book is available in hard-to-read versions for the Mac, the PC and the Windows versions of the software. For the Linux version, see http://www-cs- cmu.com/. For the Windows version, visit the Mac equivalent of this article, the Mac OS X version, which has a version that is available on the PC version of this paper and the Mac version, respectively. The full text of the paper can be downloaded from the Mac’s website at http: //www.ms-cmU.edu/.'}]
2024-01-06 19:07:49,713:INFO:Using HuggingFace's inference API to attempt to summarize the paper titled: ODIN: A Single Model for 2D and 3D Perception
2024-01-06 19:08:41,313:INFO:Raw summarization response: [{'summary_text': 'Large Language Models (LLMs) have shown to encompass a range of foundational capabilities such as commonsense and factual reasoning, world knowledge, and coherent language generation. We propose CALM—Composition to Augment Language Models. Salient features of CALM are: (i) Scales up LLMs on new tasks by ‘re-using’ existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13% on tasks like translation into English and arithmetic reasoning for low- resource languages. We see a relative improvement of 40% over the base model for code generation and explanation tasks—on-par with fully fine-tuned counterparts. The typical approach for this problem is to further pre-train or (efficiently) fine-tune the anchor model on the data that was originally used to train the augmenting model. Working with multiple distinct models is also desirable since it allows the reuse of existing models with established capabilities, providing better control and avoiding catastrophic forgetting that is prevalent in conventional approaches. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. For example, can we compose an augmenting. model’s code understanding capability with an anchor LLM’S language generation capability to enable code-to-text generation capability? The answer is yes, and we show how this can be done in the form of a Python code snippet. The code Snippet Implements the classic game of Hangman m BKey-value Mapping (HBM) mA, B, lA, lJ, mB, lK, mJ, lS, lT, lW, lM, mS, mA and mB illustrates three different capabilities with different key-value mappings. For more information on how to use CALM, please visit the CALM website or visit the Google DeepMind website or the Google Research blog. The project is open-source and funded by Google and the Google Brain Foundation. For confidential support, call the support line on 1-800-273-8255 or visit http://www.google.com/deepmind.'}]
2024-01-06 19:08:41,321:INFO:Using HuggingFace's inference API to attempt to summarize the paper titled: LLM Augmented LLMs: Expanding Capabilities through Composition
2024-01-06 19:08:41,321:INFO:Formated data written into full_papers_data Json file.
2024-01-06 19:08:41,321:INFO:Summaries written into summaries_data Json file.
2024-01-06 19:08:41,321:INFO:

 *** The script has ended *** 
 
